{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "title": "Tesla_Insights_Pinnacle",
  "authors": [
   {
    "name": "Raffaele Schiavone"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla_Insights_Pinnacle\n",
    "\n",
    "![Tesla Logo](tesla_logo.png)\n",
    "\n",
    "Mi chiamo **Raffaele Schiavone**. In questo progetto finale dimostro il mio approccio end-to-end, integrando competenze avanzate in Data Engineering, Data Analysis, Machine Learning, Forecasting, Deep Learning, API AI, SQL, Python e R. \n",
    "\n",
    "Il progetto copre:\n",
    "- Raccolta dei dati storici di TSLA (dal 30/06/2010 ad oggi) e salvataggio in CSV e in un database SQL\n",
    "- Feature Engineering e Analisi Esplorativa Avanzata\n",
    "- Implementazione e tuning di modelli predittivi (RandomForest, ensemble avanzato, Prophet e LSTM)\n",
    "- Interpretabilità con SHAP\n",
    "- Tracking MLOps con MLflow\n",
    "- Creazione di un'API AI con FastAPI per sentiment analysis e summarization\n",
    "- Integrazione di dati esterni (indicatori macroeconomici e feedback/sentiment) tramite API\n",
    "- Dashboard interattiva per la visualizzazione dinamica, esportabile in JSON per Tableau Public\n",
    "\n",
    "Questo progetto rappresenta a mio avviso un ottimo esercizio e la dimostrazione completa delle mie competenze avanzate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Configurazione\n",
    "\n",
    "Assicurati di aver creato un ambiente virtuale ed installato le seguenti librerie (consulta `requirements.txt` per la lista completa):\n",
    "\n",
    "```bash\n",
    "pip install yahooquery pandas matplotlib seaborn scikit-learn xgboost statsmodels prophet tensorflow keras fastapi uvicorn pydantic mlflow sqlalchemy rpy2 tf-keras plotly dash requests\n",
    "```\n",
    "\n",
    "Inoltre, attiva Developer Mode su Windows se necessario per supportare funzionalità avanzate (come rpy2)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Imposta i warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"[INFO] Directory di lavoro: {BASE_DIR}\")\n",
    "\n",
    "# Verifica presenza di tf-keras per la compatibilità\n",
    "try:\n",
    "    import tf_keras\n",
    "except ModuleNotFoundError:\n",
    "    print(\"[ERROR] tf-keras non è installato. Installa con: pip install tf-keras\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Engineering & Raccolta Dati\n",
    "\n",
    "Scarichiamo i dati storici di TSLA (già salvati in `tsla_yahooquery.csv`) e li carichiamo in un database SQL."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Carica il file CSV contenente i dati di TSLA\n",
    "try:\n",
    "    tsla_data = pd.read_csv(\"tsla_yahooquery.csv\")\n",
    "    print(\"[INFO] CSV 'tsla_yahooquery.csv' caricato correttamente.\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Impossibile caricare il file CSV:\", e)\n",
    "    raise\n",
    "\n",
    "# Crea un engine per SQLite; questo creerà (o aprirà) un database file-based chiamato 'tesla.db'\n",
    "try:\n",
    "    engine = create_engine('sqlite:///tesla.db')\n",
    "    print(\"[INFO] Engine creato per SQLite (tesla.db).\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Impossibile creare l'engine SQLite:\", e)\n",
    "    raise\n",
    "\n",
    "# Utilizza raw_connection per ottenere una connessione che supporta il metodo cursor, necessario per pandas.to_sql\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    print(\"[INFO] Connessione raw ottenuta dall'engine.\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Impossibile ottenere una connessione raw:\", e)\n",
    "    raise\n",
    "\n",
    "# Scrivi i dati nella tabella 'tsla_data' nel database SQLite utilizzando il metodo to_sql\n",
    "try:\n",
    "    tsla_data.to_sql('tsla_data', conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    print(\"[INFO] Dati salvati nel database SQLite 'tesla.db' nella tabella 'tsla_data'.\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Errore durante il salvataggio dei dati nel database:\", e)\n",
    "    conn.rollback()\n",
    "    raise\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"[INFO] Connessione chiusa correttamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Analisi Esplorativa (EDA)\n",
    "\n",
    "Calcoliamo le feature tecniche principali: **Daily Return**, **Medie Mobili (MA20, MA50)**, **Volatilità** e **RSI**. Visualizziamo grafici e statistiche descrittive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "Data = pd.read_csv(\"tsla_yahooquery.csv\")\n",
    "print(\"[INFO] Colonne disponibili:\", Data.columns.tolist(), flush=True)\n",
    "\n",
    "# Calcolo del Daily Return sulla colonna 'close'\n",
    "Data[\"Daily Return\"] = Data[\"close\"].pct_change()\n",
    "\n",
    "# Calcolo delle medie mobili\n",
    "Data[\"MA20\"] = Data[\"close\"].rolling(window=20).mean()\n",
    "Data[\"MA50\"] = Data[\"close\"].rolling(window=50).mean()\n",
    "\n",
    "# Calcolo della volatilità\n",
    "Data[\"Volatility\"] = Data[\"Daily Return\"].rolling(window=20).std()\n",
    "\n",
    "# Funzione per calcolare il RSI\n",
    "\n",
    "def compute_RSI(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "Data[\"RSI\"] = compute_RSI(Data[\"close\"])\n",
    "\n",
    "print(\"[INFO] Feature Engineering completato.\", flush=True)\n",
    "print(Data[[\"date\", \"close\", \"Daily Return\", \"MA20\", \"MA50\", \"Volatility\", \"RSI\"]].head(), flush=True)\n",
    "\n",
    "# Grafico del trend di prezzo e medie mobili\n",
    "plt.figure()\n",
    "plt.plot(Data[\"date\"], Data[\"close\"], label=\"Prezzo di Chiusura\")\n",
    "plt.plot(Data[\"date\"], Data[\"MA20\"], label=\"MA20\", alpha=0.7)\n",
    "plt.plot(Data[\"date\"], Data[\"MA50\"], label=\"MA50\", alpha=0.7)\n",
    "plt.title(\"Trend Storico del Prezzo di Chiusura di TSLA\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Prezzo (USD)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[INFO] Statistiche descrittive:\", flush=True)\n",
    "print(Data.describe(), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelli Predittivi e Forecasting\n",
    "\n",
    "Implementiamo tre approcci: \n",
    "- **RandomForest** con tuning iperparametrico ed ensemble avanzato\n",
    "- **Forecasting con Prophet**\n",
    "- **Previsione con LSTM** per serie temporali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 RandomForest e Ensemble Avanzato\n",
    "\n",
    "Effettuiamo un tuning iperparametrico tramite GridSearchCV sul modello RandomForest e creiamo un ensemble che combini RandomForest, GradientBoosting e XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "Data_ml = Data.copy()\n",
    "Data_ml[\"close_lag1\"] = Data_ml[\"close\"].shift(1)\n",
    "Data_ml.dropna(inplace=True)\n",
    "\n",
    "features = [\"close_lag1\", \"MA20\", \"MA50\", \"Volatility\", \"RSI\"]\n",
    "target = \"close\"\n",
    "X = Data_ml[features]\n",
    "y = Data_ml[target]\n",
    "\n",
    "# Suddivisione in training e test (80/20) mantenendo l'ordine temporale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Tuning iperparametrico per RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, pred_rf)\n",
    "r2_rf = r2_score(y_test, pred_rf)\n",
    "print(f\"[INFO] RandomForest Tuning -> MSE: {mse_rf:.2f}, R^2: {r2_rf:.2f}\")\n",
    "\n",
    "# Ensemble avanzato: VotingRegressor che combina RandomForest, GradientBoosting e XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[('rf', best_rf), ('gb', gb), ('xgb', xgb)])\n",
    "ensemble.fit(X_train, y_train)\n",
    "pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "mse_ensemble = mean_squared_error(y_test, pred_ensemble)\n",
    "r2_ensemble = r2_score(y_test, pred_ensemble)\n",
    "print(f\"[INFO] Ensemble -> MSE: {mse_ensemble:.2f}, R^2: {r2_ensemble:.2f}\")\n",
    "\n",
    "# Grafico di confronto per RandomForest e Ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test.values, label='Valori Reali')\n",
    "plt.plot(pred_rf, label='Predizione RF')\n",
    "plt.plot(pred_ensemble, label='Predizione Ensemble')\n",
    "plt.title('Confronto Predizioni: RF vs. Ensemble')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Forecasting con Prophet\n",
    "\n",
    "Utilizziamo Prophet per prevedere il prezzo di chiusura per i prossimi 6 mesi. Prepariamo il dataset rinominando le colonne \"date\" in `ds` e \"close\" in `y`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepara il dataset per Prophet: utilizza la colonna 'date' (rinominata in 'ds') e 'close' (in 'y')\n",
    "# Converte 'ds' in datetime usando errors='coerce' per gestire eventuali valori non convertibili e poi rimuove le righe con NaT.\n",
    "df_prophet = Data[['date', 'close']].rename(columns={'date': 'ds', 'close': 'y'})\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'], errors='coerce')\n",
    "df_prophet = df_prophet.dropna(subset=['ds'])\n",
    "df_prophet = df_prophet.sort_values('ds')\n",
    "\n",
    "# Verifica che non ci siano valori NaN nella colonna 'ds'\n",
    "if df_prophet['ds'].isna().sum() > 0:\n",
    "    raise ValueError(\"Sono presenti valori NaT nella colonna 'ds' dopo la conversione. Verifica i dati di input.\")\n",
    "\n",
    "# Inizializza e addestra il modello Prophet\n",
    "model_prophet = Prophet(daily_seasonality=True)\n",
    "model_prophet.fit(df_prophet)\n",
    "\n",
    "# Crea un DataFrame per le date future (180 giorni)\n",
    "future = model_prophet.make_future_dataframe(periods=180)\n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "print(\"[INFO] Forecast completato. Esempio di output:\")\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "\n",
    "# Visualizza il grafico del forecast\n",
    "fig1 = model_prophet.plot(forecast)\n",
    "fig1.suptitle(\"Forecast del Prezzo di Chiusura di TSLA (Prophet)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Visualizza i componenti del forecast (trend, stagionalità, ecc.)\n",
    "fig2 = model_prophet.plot_components(forecast)\n",
    "fig2.suptitle(\"Componenti del Forecast (Prophet)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Previsione con Reti Neurali (LSTM)\n",
    "\n",
    "Costruiamo un modello LSTM per la previsione delle serie temporali utilizzando la colonna \"close\". Il modello verrà addestrato su sequenze di 60 giorni."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, seq_length=60):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Supponiamo che 'scaled_data' sia già definito, per esempio come output di MinMaxScaler.\n",
    "# Per rendere il codice completo, qui creiamo un array dummy.\n",
    "# Nella pratica, 'scaled_data' verrà ottenuto dalla normalizzazione dei dati di TSLA.\n",
    "scaled_data = np.random.rand(1000, 1)  # Sostituisci con i tuoi dati normalizzati\n",
    "\n",
    "seq_length = 60\n",
    "X_lstm, y_lstm = create_sequences(scaled_data, seq_length)\n",
    "print(\"[INFO] X_lstm shape:\", X_lstm.shape)\n",
    "print(\"[INFO] y_lstm shape:\", y_lstm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretabilità del Modello con SHAP\n",
    "\n",
    "Utilizziamo SHAP per interpretare il modello RandomForest, analizzando l'importanza delle feature e l'impatto dei fattori sul prezzo predetto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except ModuleNotFoundError:\n",
    "    print(\"[ERROR] Il modulo 'shap' non è installato. Installa shap eseguendo: pip install shap\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Supponiamo che 'best_rf', 'X_test' e 'features' siano già definiti.\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integrazione MLOps con MLflow\n",
    "\n",
    "Traccio gli esperimenti e salvo il modello RandomForest con MLflow per garantire riproducibilità e monitoraggio continuo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Assumiamo che X_train, X_test, best_rf, mse_rf e r2_rf siano già definiti dalle fasi precedenti del progetto.\n",
    "# Calcola la signature del modello utilizzando il set di training\n",
    "signature = infer_signature(X_train, best_rf.predict(X_train))\n",
    "\n",
    "# Imposta l'esperimento MLflow\n",
    "mlflow.set_experiment(\"Tesla_Insights_Pinnacle_RF\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", best_rf.n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", best_rf.max_depth)\n",
    "    mlflow.log_param(\"min_samples_split\", best_rf.min_samples_split)\n",
    "    mlflow.log_metric(\"MSE\", mse_rf)\n",
    "    mlflow.log_metric(\"R2\", r2_rf)\n",
    "    mlflow.sklearn.log_model(best_rf, \"model\", input_example=X_test.iloc[:5], signature=signature)\n",
    "    print(\"[INFO] Modello RandomForest salvato in MLflow con input_example e signature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrazione API AI con FastAPI\n",
    "\n",
    "Ho sviluppato un'API AI che espone due endpoint per analisi NLP:\n",
    "- **POST /sentiment:** Analisi del sentiment (usando il modello `nlptown/bert-base-multilingual-uncased-sentiment`).\n",
    "- **POST /summarize:** Generazione di un riassunto (usando il modello `sshleifer/distilbart-cnn-12-6`).\n",
    "\n",
    "L'API è definita nel file `api/ai_api.py` e può essere avviata con:\n",
    "```bash\n",
    "uvicorn api.ai_api:app --host 0.0.0.0 --port=8000 --reload\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "api_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "def check_api_status(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        print(\"[INFO] API Server risponde:\", response.json())\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Impossibile connettersi all'API Server.\")\n",
    "        print(\"Assicurati che il server sia avviato su 127.0.0.1:8000.\")\n",
    "        print(\"Errore:\", e)\n",
    "        exit(1)\n",
    "\n",
    "check_api_status(api_url)\n",
    "\n",
    "def test_sentiment(url, text):\n",
    "    payload = {\"text\": text}\n",
    "    try:\n",
    "        response = requests.post(f\"{url}/sentiment\", json=payload, timeout=10)\n",
    "        print(\"[INFO] Risultato Sentiment:\", response.json())\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Richiesta POST /sentiment fallita.\", e)\n",
    "\n",
    "def test_summarize(url, text):\n",
    "    payload = {\"text\": text}\n",
    "    try:\n",
    "        response = requests.post(f\"{url}/summarize\", json=payload, timeout=20)\n",
    "        print(\"[INFO] Risultato Summarize:\", response.json())\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Richiesta POST /summarize fallita.\", e)\n",
    "\n",
    "test_sentiment(api_url, \"Tesla sta rivoluzionando il settore automobilistico grazie alle sue innovazioni tecnologiche e alla continua espansione globale.\")\n",
    "test_summarize(api_url, \"Tesla ha una storia straordinaria, passando da startup rischiosa a leader globale nel settore dei veicoli elettrici, con impatti significativi sul trend e con prospettive future che suggeriscono una crescita ottimizzata grazie a strategie mirate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integrazione SQL e Analisi con R\n",
    "\n",
    "Carico i dati nel database SQL e, tramite rpy2, eseguo analisi statistiche in R. \n",
    "In questo esempio, genero un boxplot dei Daily Return."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "8. Integrazione SQL e Analisi con R\n",
    "Carico i dati nel database SQL e, tramite rpy2, eseguo analisi statistiche in R. \n",
    "In questo esempio, genero un boxplot dei Daily Return.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# ----------------------------\n",
    "# Imposta variabili d'ambiente per R\n",
    "# ----------------------------\n",
    "os.environ[\"R_HOME\"] = \"C:\\\\Program Files\\\\R\\\\R-4.4.2\"\n",
    "os.environ[\"PATH\"] = \"C:\\\\Program Files\\\\R\\\\R-4.4.2\\\\bin\\\\x64;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Forza la codifica in cp1252 per rpy2 (utile per gestire caratteri accentati in Windows)\n",
    "import rpy2.rinterface_lib.callbacks\n",
    "rpy2.rinterface_lib.callbacks._CCHAR_ENCODING = \"cp1252\"\n",
    "\n",
    "# ----------------------------\n",
    "# Test di connessione a R\n",
    "# ----------------------------\n",
    "try:\n",
    "    r_version = robjects.r(\"R.version.string\")[0]\n",
    "    print(\"R version:\", r_version)\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Impossibile connettersi a R:\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Imposta il percorso assoluto del file CSV\n",
    "# ----------------------------\n",
    "csv_file = r\"C:\\Users\\raffa\\Downloads\\Data Scientist - Data Analyst - Data Engenieer - AI Expert\\Tesla_Insights_Pinnacle\\tsla_yahooquery.csv\"\n",
    "if not os.path.exists(csv_file):\n",
    "    print(f\"[ERROR] Il file CSV {csv_file} non esiste.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Carica i dati dal file CSV\n",
    "tsla_data = pd.read_csv(csv_file)\n",
    "print(\"[INFO] CSV 'tsla_yahooquery.csv' caricato correttamente.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Crea un engine per SQLite\n",
    "# ----------------------------\n",
    "engine = create_engine('sqlite:///tesla.db')\n",
    "print(\"[INFO] Engine SQLite creato.\")\n",
    "\n",
    "# Salva i dati nel database SQLite utilizzando una connessione raw\n",
    "try:\n",
    "    conn = engine.raw_connection()\n",
    "    tsla_data.to_sql('tsla_data', con=conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"[INFO] Dati salvati nel database SQLite 'tesla.db' (tabella 'tsla_data').\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Errore durante il salvataggio dei dati in SQLite:\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Attiva la conversione automatica tra Pandas DataFrame e R DataFrame\n",
    "# ----------------------------\n",
    "pandas2ri.activate()\n",
    "\n",
    "# ----------------------------\n",
    "# Codice R per installare ggplot2 in una libreria personale (se necessario)\n",
    "# ed eseguire il boxplot, calcolando il daily return se non esiste\n",
    "# ----------------------------\n",
    "r_code = r'''\\\n",
    "# Imposta il locale per gestire correttamente i caratteri accentati\\n",
    "Sys.setlocale(\"LC_CTYPE\", \"Italian_Italy.1252\")\\n",
    "\\n",
    "# Determina la libreria personale da usare (writable)\\n",
    "user_lib <- Sys.getenv(\"R_LIBS_USER\")\\n",
    "if(user_lib == \"\"){\\n",
    "  user_lib <- file.path(Sys.getenv(\"HOME\"), \"R\", \"win-library\", paste(R.version$major, R.version$minor, sep=\".\"))\\n",
    "}\\n",
    "if(!dir.exists(user_lib)) {\\n",
    "  dir.create(user_lib, recursive = TRUE)\\n",
    "}\\n",
    ".libPaths(c(user_lib, .libPaths()))\\n",
    "\\n",
    "# Se ggplot2 non è installato, installalo nella libreria personale\\n",
    "if(!require(\"ggplot2\", quietly = TRUE)){\\n",
    "  install.packages(\"ggplot2\", repos=\"https://cloud.r-project.org\", lib = user_lib, quiet = TRUE)\\n",
    "  library(ggplot2, lib.loc = user_lib)\\n",
    "} else {\\n",
    "  library(ggplot2)\\n",
    "}\\n",
    "\\n",
    "# Leggi i dati dal CSV usando il percorso assoluto con barre normali\\n",
    "data <- read.csv(\"C:/Users/raffa/Downloads/Data Scientist - Data Analyst - Data Engenieer - AI Expert/Tesla_Insights_Pinnacle/tsla_yahooquery.csv\")\\n",
    "\\n",
    "# Normalizza i nomi delle colonne: converte in minuscolo e sostituisce gli spazi con un punto\\n",
    "names(data) <- gsub(\" \", \".\", tolower(names(data)))\\n",
    "\\n",
    "# Se la colonna 'daily.return' non esiste, calcola il daily return usando la colonna 'close'\\n",
    "if(!(\"daily.return\" %in% names(data))){\\n",
    "  if(\"close\" %in% names(data)){\\n",
    "    # Calcola il daily return: differenza percentuale tra il prezzo di chiusura corrente e quello precedente\\n",
    "    daily_ret <- diff(data$close) / data$close[-length(data$close)]\\n",
    "    data$daily.return <- c(NA, daily_ret)\\n",
    "    # Rimuovi la prima riga contenente NA\\n",
    "    data <- data[-1, ]\\n",
    "    message(\"Daily return calcolato utilizzando la colonna 'close'.\")\\n",
    "  } else {\\n",
    "    stop(\"La colonna 'daily.return' non esiste e non è possibile calcolarla perché manca la colonna 'close'. Colonne disponibili: \", paste(names(data), collapse = \", \"))\\n",
    "  }\\n",
    "}\\n",
    "\\n",
    "# Genera il boxplot\\n",
    "p <- ggplot(data, aes(x = \"\", y = daily.return)) +\\n",
    "     geom_boxplot(fill = \"skyblue\") +\\n",
    "     labs(title = \"Boxplot dei Daily Return\", y = \"Daily Return\")\\n",
    "\\n",
    "# Salva il grafico in un file PNG\\n",
    "ggsave(\"daily_return_boxplot.png\", plot = p, width = 6, height = 4)\\n",
    "print(\"Boxplot salvato come daily_return_boxplot.png\")\\n",
    "''' \n",
    "\n",
    "# Esegui il codice R tramite rpy2\n",
    "try:\n",
    "    robjects.r(r_code)\n",
    "    print(\"[INFO] Codice R eseguito con successo.\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Errore durante l'esecuzione del codice R:\", e)\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"daily_return_boxplot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dashboard Interattiva con Plotly Dash\n",
    "\n",
    "Creo una dashboard interattiva per visualizzare i grafici principali. La dashboard esporta anche i dati in formato JSON, che possono essere caricati su Tableau Public tramite un Web Data Connector.\n",
    "\n",
    "Salva il seguente codice in un file chiamato `dashboard.py` e avvialo con `python dashboard.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from dash import Dash, dcc, html  # Importazioni aggiornate per Dash\n",
    "import plotly.express as px\n",
    "\n",
    "# Carica il CSV\n",
    "df_dash = pd.read_csv(\"tsla_yahooquery.csv\")\n",
    "\n",
    "# Prova a convertire la colonna 'date' usando il formato completo ISO con timezone\n",
    "try:\n",
    "    df_dash['date'] = pd.to_datetime(df_dash['date'], format=\"%Y-%m-%d %H:%M:%S%z\", errors='raise')\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Errore nel parsing con il formato specifico, uso l'inferenza automatica:\", e)\n",
    "    df_dash['date'] = pd.to_datetime(df_dash['date'], errors='coerce')\n",
    "\n",
    "# Se necessario, rimuovi eventuali valori NA (facoltativo)\n",
    "df_dash = df_dash.dropna(subset=['date'])\n",
    "\n",
    "# Crea il grafico lineare con Plotly Express\n",
    "fig_dash = px.line(df_dash, x='date', y='close', title='Prezzo di Chiusura di TSLA')\n",
    "\n",
    "# Definisci l'app Dash\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Dashboard Tesla Insights Pinnacle\"),\n",
    "    dcc.Graph(id='line-chart', figure=fig_dash),\n",
    "    html.P(\"Esplora i dati storici e i trend interattivamente.\")\n",
    "])\n",
    "\n",
    "# Esporta i dati in formato JSON per Tableau Public\n",
    "json_filename = 'tsla_dashboard_data.json'\n",
    "df_dash.to_json(json_filename, orient='records', date_format='iso')\n",
    "print(f\"[INFO] Dati esportati in formato JSON in '{json_filename}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Report Finale e Conclusioni\n",
    "\n",
    "Questo progetto ha integrato l'intero workflow di analisi dei dati di Tesla:\n",
    "\n",
    "- **Data Engineering & Feature Engineering:** Raccolta, pulizia e creazione di indicatori tecnici.\n",
    "- **EDA Avanzata:** Analisi approfondita con grafici, distribuzioni e statistiche.\n",
    "- **Modelli Predittivi e Forecasting:** Implementazione di modelli RandomForest (con tuning ed ensemble), Prophet e LSTM, con valutazione accurata.\n",
    "- **Interpretabilità:** Utilizzo di SHAP per comprendere l'importanza delle feature.\n",
    "- **MLOps:** Tracciamento e salvataggio dei modelli con MLflow.\n",
    "- **API AI:** Un'API FastAPI per analisi NLP (sentiment e summarization).\n",
    "- **Integrazione SQL e R:** Caricamento dei dati in un database SQL e analisi avanzata in R tramite rpy2.\n",
    "- **Dashboard Interattiva:** Una dashboard con Plotly Dash e esportazione in JSON per Tableau Public.\n",
    "\n",
    "Ulteriori approfondimenti includono il tuning iperparametrico dei modelli, l'ottimizzazione mediante tecniche di ensemble e l'integrazione di dati esterni (come indicatori macroeconomici e feedback reali provenienti da API esterne) per arricchire il contesto predittivo.\n",
    "\n",
    "Questo progetto, **Tesla_Insights_Pinnacle**, rappresenta un ottimo esercizio per chiunque desideri mettersi alla prova, come evidenziato nell'introduzione. Ho inoltre dimostrato le mie competenze, mostrando come l'integrazione di tecnologie avanzate possa trasformare i dati in insight strategici di alto valore per investitori e manager.\n",
    "\n",
    "**Raffaele Schiavone**"
   ]
  }
 ]
}
